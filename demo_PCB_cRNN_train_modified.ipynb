{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Occupy a GPU for the model to be loaded \n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "# GPU ID, if occupied change to an available GPU ID listed under !nvidia-smi\n",
    "%env CUDA_VISIBLE_DEVICES=0'''\n",
    "\n",
    "import numpy as np\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "import h5py\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "from ddc_pub import ddc_v3 as ddc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors(binmols_list, qsar_model=None):\n",
    "    \"\"\"Calculate molecular descriptors of SMILES in a list.\n",
    "    The descriptors are logp, tpsa, mw, qed, hba, hbd and probability of being active towards DRD2.\n",
    "    \n",
    "    Returns:\n",
    "        A np.ndarray of descriptors.\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "    import rdkit\n",
    "    from rdkit import Chem, DataStructs\n",
    "    from rdkit.Chem import Descriptors, rdMolDescriptors, AllChem, QED\n",
    "    \n",
    "    descriptors = []\n",
    "    active_mols = []\n",
    "    \n",
    "    for idx, binmol in enumerate(binmols_list):\n",
    "        mol = Chem.Mol(binmol)\n",
    "        if mol:\n",
    "            try:\n",
    "                logp  = Descriptors.MolLogP(mol)\n",
    "                tpsa  = Descriptors.TPSA(mol)\n",
    "                molwt = Descriptors.ExactMolWt(mol)\n",
    "                hba   = rdMolDescriptors.CalcNumHBA(mol)\n",
    "                hbd   = rdMolDescriptors.CalcNumHBD(mol)\n",
    "                qed   = QED.qed(mol)\n",
    "                \n",
    "                fp = AllChem.GetMorganFingerprintAsBitVect(mol,2, nBits=2048)\n",
    "                ecfp4 = np.zeros((2048,))\n",
    "                DataStructs.ConvertToNumpyArray(fp, ecfp4) \n",
    "                active = qsar_model.predict_proba([ecfp4])[0][1]\n",
    "                descriptors.append([logp, tpsa, molwt, qed, hba, hbd, active]) \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"Invalid generation.\")\n",
    "            \n",
    "    return np.asarray(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Anaconda3\\envs\\ddc_env\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator SVC from version 0.20.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load QSAR model\n",
    "qsar_model_name = \"models/qsar_model.pickle\"\n",
    "with open(qsar_model_name, \"rb\") as file:\n",
    "    qsar_model = pickle.load(file)[\"classifier_sv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_filename = r\"C:\\Users\\Leave\\OneDrive - hust.edu.cn\\大创\\pcko1-Deep-Drug-Coder-d6e7ef3\\datasets\\CHEMBL25_TRAIN_MOLS.h5\"\n",
    "with h5py.File(dataset_filename, \"r\") as f:\n",
    "    binmols = f[\"mols\"][0:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the descriptors for the molecules in the dataset\n",
    "# This process takes a lot of time and it's good if the descriptors are\n",
    "# pre-calculated and stored in a file to load every time\n",
    "descr = get_descriptors(binmols, qsar_model=qsar_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All apriori known characters of the SMILES in the dataset\n",
    "charset = \"Brc1(-23[nH])45C=NOso#FlS67+89%0\"\n",
    "# Apriori known max length of the SMILES in the dataset\n",
    "maxlen = 128\n",
    "# Name of the dataset\n",
    "name = \"ChEMBL25_TRAIN\"\n",
    "\n",
    "dataset_info = {\"charset\": charset, \"maxlen\": maxlen, \"name\": name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model in train mode.\n",
      "Input type is 'molecular descriptors'.\n",
      "Applying scaling on input.\n",
      "Model received 230 train samples and 26 validation samples.\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Latent_Input (InputLayer)       [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Inputs (InputLayer)     [(None, 142, 35)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "latent_to_states_model (Model)  [(None, 512), (None, 36864       Latent_Input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_model (Model)             (None, 142, 35)      5350947     Decoder_Inputs[0][0]             \n",
      "                                                                 latent_to_states_model[1][0]     \n",
      "                                                                 latent_to_states_model[1][1]     \n",
      "                                                                 latent_to_states_model[1][2]     \n",
      "                                                                 latent_to_states_model[1][3]     \n",
      "                                                                 latent_to_states_model[1][4]     \n",
      "                                                                 latent_to_states_model[1][5]     \n",
      "==================================================================================================\n",
      "Total params: 5,387,811\n",
      "Trainable params: 5,378,595\n",
      "Non-trainable params: 9,216\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initialize a model\n",
    "model = ddc.DDC(x              = descr,        # input\n",
    "                y              = binmols,      # output\n",
    "                dataset_info   = dataset_info, # dataset information\n",
    "                scaling        = True,         # scale the descriptors\n",
    "                noise_std      = 0.1,          # std of the noise layer\n",
    "                lstm_dim       = 512,          # breadth of LSTM layers\n",
    "                dec_layers     = 3,            # number of decoding layers\n",
    "                batch_size     = 26)          # batch size for training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "\n",
      "Model trained with dataset ChEMBL25_TRAIN that has maxlen=138 and charset=Brc1(-23[nH])45C=NOso#FlS67+89%0 for 30 epochs.\n",
      "noise_std: 0.100000, lstm_dim: 512, dec_layers: 3, td_dense_dim: 0, batch_size: 26, codelayer_dim: 7, lr: 0.001000.\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 1/300\n",
      "1/0 - 11s - loss: 4.1368 - val_loss: 31.7256\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 2/300\n",
      "1/0 - 0s - loss: 2.5051 - val_loss: 32.7391\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 3/300\n",
      "1/0 - 0s - loss: 1.9811 - val_loss: 34.7723\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 4/300\n",
      "1/0 - 0s - loss: 2.1096 - val_loss: 31.7657\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 5/300\n",
      "1/0 - 0s - loss: 1.6876 - val_loss: 29.8915\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 6/300\n",
      "1/0 - 0s - loss: 1.4026 - val_loss: 30.1978\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 7/300\n",
      "1/0 - 0s - loss: 1.1738 - val_loss: 31.5801\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 8/300\n",
      "1/0 - 0s - loss: 1.0319 - val_loss: 34.2359\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 9/300\n",
      "1/0 - 0s - loss: 1.0476 - val_loss: 35.1920\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 10/300\n",
      "1/0 - 0s - loss: 0.9954 - val_loss: 34.1964\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 11/300\n",
      "1/0 - 0s - loss: 1.0125 - val_loss: 30.4540\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 12/300\n",
      "1/0 - 0s - loss: 0.8818 - val_loss: 28.7522\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 13/300\n",
      "1/0 - 0s - loss: 0.9953 - val_loss: 25.4483\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 14/300\n",
      "1/0 - 0s - loss: 0.9394 - val_loss: 22.4504\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 15/300\n",
      "1/0 - 0s - loss: 0.9326 - val_loss: 20.3376\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 16/300\n",
      "1/0 - 0s - loss: 0.9194 - val_loss: 17.8979\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 17/300\n",
      "1/0 - 1s - loss: 0.9198 - val_loss: 14.9325\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 18/300\n",
      "1/0 - 0s - loss: 0.8652 - val_loss: 13.0075\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 19/300\n",
      "1/0 - 0s - loss: 0.8731 - val_loss: 11.9459\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 20/300\n",
      "1/0 - 0s - loss: 0.9487 - val_loss: 11.0425\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 21/300\n",
      "1/0 - 0s - loss: 0.8601 - val_loss: 10.6270\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 22/300\n",
      "1/0 - 0s - loss: 0.8688 - val_loss: 10.1951\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 23/300\n",
      "1/0 - 0s - loss: 0.8961 - val_loss: 9.9103\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 24/300\n",
      "1/0 - 0s - loss: 0.8671 - val_loss: 9.7159\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 25/300\n",
      "1/0 - 0s - loss: 0.8712 - val_loss: 9.6303\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 26/300\n",
      "1/0 - 0s - loss: 0.8193 - val_loss: 9.5433\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 27/300\n",
      "1/0 - 0s - loss: 0.9049 - val_loss: 9.2729\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 28/300\n",
      "1/0 - 0s - loss: 0.8034 - val_loss: 9.1001\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 29/300\n",
      "1/0 - 0s - loss: 0.9279 - val_loss: 8.8241\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 30/300\n",
      "1/0 - 0s - loss: 0.8433 - val_loss: 8.6527\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 31/300\n",
      "1/0 - 0s - loss: 0.8242 - val_loss: 8.6060\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 32/300\n",
      "1/0 - 0s - loss: 0.8468 - val_loss: 8.7715\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 33/300\n",
      "1/0 - 0s - loss: 0.8166 - val_loss: 9.0012\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 34/300\n",
      "1/0 - 0s - loss: 0.7662 - val_loss: 9.2446\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 35/300\n",
      "1/0 - 0s - loss: 0.8173 - val_loss: 8.8771\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 36/300\n",
      "1/0 - 0s - loss: 0.8433 - val_loss: 8.4607\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 37/300\n",
      "1/0 - 0s - loss: 0.8928 - val_loss: 7.9394\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 38/300\n",
      "1/0 - 0s - loss: 0.8133 - val_loss: 7.7249\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 39/300\n",
      "1/0 - 0s - loss: 0.8735 - val_loss: 7.7506\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 40/300\n",
      "1/0 - 0s - loss: 0.9425 - val_loss: 7.6674\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 41/300\n",
      "1/0 - 0s - loss: 0.8489 - val_loss: 7.6597\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 42/300\n",
      "1/0 - 0s - loss: 0.8419 - val_loss: 7.7833\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 43/300\n",
      "1/0 - 0s - loss: 0.8359 - val_loss: 7.8964\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 44/300\n",
      "1/0 - 0s - loss: 0.8065 - val_loss: 7.9609\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 45/300\n",
      "1/0 - 0s - loss: 0.8016 - val_loss: 7.8470\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 46/300\n",
      "1/0 - 1s - loss: 0.7848 - val_loss: 7.7147\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 47/300\n",
      "1/0 - 0s - loss: 0.7761 - val_loss: 7.5163\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 48/300\n",
      "1/0 - 1s - loss: 0.7954 - val_loss: 7.3121\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 49/300\n",
      "1/0 - 0s - loss: 0.7816 - val_loss: 7.2996\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 50/300\n",
      "Model saved in new_model--50--7.2897--0.0010000.\n",
      "1/0 - 2s - loss: 0.7761 - val_loss: 7.2897\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 51/300\n",
      "1/0 - 0s - loss: 0.7671 - val_loss: 7.2639\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 52/300\n",
      "1/0 - 0s - loss: 0.7581 - val_loss: 7.1167\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 53/300\n",
      "1/0 - 0s - loss: 0.7959 - val_loss: 7.2665\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 54/300\n",
      "1/0 - 0s - loss: 0.7345 - val_loss: 7.2370\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 55/300\n",
      "1/0 - 0s - loss: 0.7344 - val_loss: 7.0764\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/0 - 1s - loss: 0.8029 - val_loss: 6.9959\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 57/300\n",
      "1/0 - 0s - loss: 0.8077 - val_loss: 7.0209\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 58/300\n",
      "1/0 - 0s - loss: 0.7073 - val_loss: 7.1531\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 59/300\n",
      "1/0 - 0s - loss: 0.7677 - val_loss: 6.8854\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 60/300\n",
      "1/0 - 1s - loss: 0.7129 - val_loss: 6.6967\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 61/300\n",
      "1/0 - 0s - loss: 0.7792 - val_loss: 6.9234\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 62/300\n",
      "1/0 - 0s - loss: 0.7250 - val_loss: 6.7923\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 63/300\n",
      "1/0 - 0s - loss: 0.6978 - val_loss: 6.6935\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 64/300\n",
      "1/0 - 0s - loss: 0.6298 - val_loss: 6.8886\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 65/300\n",
      "1/0 - 0s - loss: 0.7601 - val_loss: 6.5959\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 66/300\n",
      "1/0 - 0s - loss: 0.6872 - val_loss: 6.3471\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 67/300\n",
      "1/0 - 0s - loss: 0.6868 - val_loss: 6.5657\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 68/300\n",
      "1/0 - 0s - loss: 0.7386 - val_loss: 6.7115\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 69/300\n",
      "1/0 - 0s - loss: 0.6740 - val_loss: 6.6236\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 70/300\n",
      "1/0 - 0s - loss: 0.7112 - val_loss: 6.4996\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 71/300\n",
      "1/0 - 0s - loss: 0.6650 - val_loss: 6.1671\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 72/300\n",
      "1/0 - 0s - loss: 0.6470 - val_loss: 6.0873\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 73/300\n",
      "1/0 - 0s - loss: 0.7081 - val_loss: 6.0603\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 74/300\n",
      "1/0 - 0s - loss: 0.6286 - val_loss: 6.3148\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 75/300\n",
      "1/0 - 0s - loss: 0.6640 - val_loss: 6.2632\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 76/300\n",
      "1/0 - 0s - loss: 0.6998 - val_loss: 6.0966\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 77/300\n",
      "1/0 - 0s - loss: 0.6962 - val_loss: 5.8843\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 78/300\n",
      "1/0 - 0s - loss: 0.6550 - val_loss: 6.0059\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 79/300\n",
      "1/0 - 0s - loss: 0.6170 - val_loss: 6.0273\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 80/300\n",
      "1/0 - 1s - loss: 0.6285 - val_loss: 5.8910\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 81/300\n",
      "1/0 - 0s - loss: 0.6607 - val_loss: 5.7907\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 82/300\n",
      "1/0 - 0s - loss: 0.6542 - val_loss: 5.7672\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 83/300\n",
      "1/0 - 0s - loss: 0.6317 - val_loss: 5.7944\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 84/300\n",
      "1/0 - 0s - loss: 0.6148 - val_loss: 5.8555\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 85/300\n",
      "1/0 - 0s - loss: 0.6218 - val_loss: 5.7110\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 86/300\n",
      "1/0 - 0s - loss: 0.6796 - val_loss: 5.7100\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 87/300\n",
      "1/0 - 0s - loss: 0.6392 - val_loss: 5.9397\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 88/300\n",
      "1/0 - 0s - loss: 0.6158 - val_loss: 6.1930\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 89/300\n",
      "1/0 - 0s - loss: 0.6220 - val_loss: 5.9166\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 90/300\n",
      "1/0 - 0s - loss: 0.6060 - val_loss: 5.7509\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 91/300\n",
      "1/0 - 0s - loss: 0.7103 - val_loss: 5.8505\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 92/300\n",
      "1/0 - 0s - loss: 0.7472 - val_loss: 6.1485\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 93/300\n",
      "1/0 - 0s - loss: 0.6207 - val_loss: 6.0173\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 94/300\n",
      "1/0 - 0s - loss: 0.6477 - val_loss: 5.9278\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 95/300\n",
      "1/0 - 0s - loss: 0.6188 - val_loss: 6.0562\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 96/300\n",
      "1/0 - 0s - loss: 0.6368 - val_loss: 5.7754\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 97/300\n",
      "1/0 - 0s - loss: 0.6493 - val_loss: 5.7788\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 98/300\n",
      "1/0 - 0s - loss: 0.6261 - val_loss: 5.6448\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 99/300\n",
      "1/0 - 0s - loss: 0.5843 - val_loss: 5.6749\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 100/300\n",
      "Model saved in new_model--100--5.6005--0.0010000.\n",
      "1/0 - 2s - loss: 0.6223 - val_loss: 5.6005\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 101/300\n",
      "1/0 - 0s - loss: 0.5779 - val_loss: 5.5145\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 102/300\n",
      "1/0 - 0s - loss: 0.6183 - val_loss: 5.7041\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 103/300\n",
      "1/0 - 0s - loss: 0.6729 - val_loss: 5.8916\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 104/300\n",
      "1/0 - 0s - loss: 0.6374 - val_loss: 5.8452\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 105/300\n",
      "1/0 - 0s - loss: 0.6563 - val_loss: 5.7640\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 106/300\n",
      "1/0 - 0s - loss: 0.5940 - val_loss: 5.5709\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 107/300\n",
      "1/0 - 0s - loss: 0.6301 - val_loss: 5.5065\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 108/300\n",
      "1/0 - 0s - loss: 0.5867 - val_loss: 5.6253\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 109/300\n",
      "1/0 - 0s - loss: 0.5921 - val_loss: 5.5108\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 110/300\n",
      "1/0 - 0s - loss: 0.6011 - val_loss: 5.4014\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 111/300\n",
      "1/0 - 0s - loss: 0.6116 - val_loss: 5.3309\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 112/300\n",
      "1/0 - 0s - loss: 0.6095 - val_loss: 5.4320\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 113/300\n",
      "1/0 - 0s - loss: 0.6242 - val_loss: 5.3183\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 114/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/0 - 0s - loss: 0.6181 - val_loss: 5.3960\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 115/300\n",
      "1/0 - 0s - loss: 0.5802 - val_loss: 5.5236\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 116/300\n",
      "1/0 - 0s - loss: 0.6404 - val_loss: 5.5349\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 117/300\n",
      "1/0 - 0s - loss: 0.5765 - val_loss: 5.4436\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 118/300\n",
      "1/0 - 0s - loss: 0.6015 - val_loss: 5.6332\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 119/300\n",
      "1/0 - 0s - loss: 0.6114 - val_loss: 5.6094\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 120/300\n",
      "1/0 - 0s - loss: 0.5762 - val_loss: 5.4248\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 121/300\n",
      "1/0 - 0s - loss: 0.5815 - val_loss: 5.3725\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 122/300\n",
      "1/0 - 0s - loss: 0.6027 - val_loss: 5.3426\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 123/300\n",
      "1/0 - 0s - loss: 0.5850 - val_loss: 5.3902\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 124/300\n",
      "1/0 - 0s - loss: 0.5631 - val_loss: 5.4702\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 125/300\n",
      "1/0 - 0s - loss: 0.5876 - val_loss: 5.5657\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 126/300\n",
      "1/0 - 0s - loss: 0.5939 - val_loss: 5.6730\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 127/300\n",
      "1/0 - 0s - loss: 0.5705 - val_loss: 5.6438\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 128/300\n",
      "1/0 - 0s - loss: 0.6379 - val_loss: 5.7604\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 129/300\n",
      "1/0 - 0s - loss: 0.5975 - val_loss: 5.9574\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 130/300\n",
      "1/0 - 0s - loss: 0.6190 - val_loss: 6.0301\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 131/300\n",
      "1/0 - 0s - loss: 0.6625 - val_loss: 5.9499\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 132/300\n",
      "1/0 - 0s - loss: 0.5716 - val_loss: 5.6604\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 133/300\n",
      "1/0 - 0s - loss: 0.6087 - val_loss: 5.3579\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 134/300\n",
      "1/0 - 0s - loss: 0.5274 - val_loss: 5.1274\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 135/300\n",
      "1/0 - 0s - loss: 0.5629 - val_loss: 5.1956\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 136/300\n",
      "1/0 - 0s - loss: 0.5427 - val_loss: 5.4221\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 137/300\n",
      "1/0 - 0s - loss: 0.5253 - val_loss: 5.4766\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 138/300\n",
      "1/0 - 0s - loss: 0.6223 - val_loss: 5.3712\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 139/300\n",
      "1/0 - 0s - loss: 0.5594 - val_loss: 5.2306\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 140/300\n",
      "1/0 - 0s - loss: 0.5341 - val_loss: 5.2767\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 141/300\n",
      "1/0 - 0s - loss: 0.5657 - val_loss: 5.2252\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 142/300\n",
      "1/0 - 0s - loss: 0.6026 - val_loss: 5.4218\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 143/300\n",
      "1/0 - 0s - loss: 0.5607 - val_loss: 5.3311\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 144/300\n",
      "1/0 - 0s - loss: 0.5515 - val_loss: 5.3551\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 145/300\n",
      "1/0 - 0s - loss: 0.5461 - val_loss: 5.0708\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 146/300\n",
      "1/0 - 0s - loss: 0.5465 - val_loss: 5.0289\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 147/300\n",
      "1/0 - 0s - loss: 0.5490 - val_loss: 5.0023\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 148/300\n",
      "1/0 - 0s - loss: 0.5806 - val_loss: 5.1887\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 149/300\n",
      "1/0 - 0s - loss: 0.5145 - val_loss: 5.1507\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 150/300\n",
      "Model saved in new_model--150--5.0306--0.0010000.\n",
      "1/0 - 2s - loss: 0.5063 - val_loss: 5.0306\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 151/300\n",
      "1/0 - 0s - loss: 0.5120 - val_loss: 5.1430\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 152/300\n",
      "1/0 - 0s - loss: 0.6613 - val_loss: 5.0593\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 153/300\n",
      "1/0 - 0s - loss: 0.5048 - val_loss: 5.0726\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 154/300\n",
      "1/0 - 0s - loss: 0.5432 - val_loss: 5.1352\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 155/300\n",
      "1/0 - 0s - loss: 0.5997 - val_loss: 5.2212\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 156/300\n",
      "1/0 - 0s - loss: 0.5821 - val_loss: 5.1997\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 157/300\n",
      "1/0 - 0s - loss: 0.5249 - val_loss: 5.0408\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 158/300\n",
      "1/0 - 0s - loss: 0.5951 - val_loss: 4.9990\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 159/300\n",
      "1/0 - 0s - loss: 0.5550 - val_loss: 4.9931\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 160/300\n",
      "1/0 - 0s - loss: 0.5326 - val_loss: 5.0847\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 161/300\n",
      "1/0 - 0s - loss: 0.5467 - val_loss: 5.1783\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 162/300\n",
      "1/0 - 0s - loss: 0.4997 - val_loss: 5.1871\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 163/300\n",
      "1/0 - 0s - loss: 0.5079 - val_loss: 5.3362\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 164/300\n",
      "1/0 - 0s - loss: 0.6940 - val_loss: 4.9340\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 165/300\n",
      "1/0 - 0s - loss: 0.5021 - val_loss: 4.8830\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 166/300\n",
      "1/0 - 0s - loss: 0.5793 - val_loss: 5.2845\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 167/300\n",
      "1/0 - 0s - loss: 0.5816 - val_loss: 5.4319\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 168/300\n",
      "1/0 - 0s - loss: 0.5463 - val_loss: 5.8436\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 169/300\n",
      "1/0 - 0s - loss: 0.5425 - val_loss: 5.7332\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 170/300\n",
      "1/0 - 0s - loss: 0.5624 - val_loss: 5.1985\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 171/300\n",
      "1/0 - 0s - loss: 0.5302 - val_loss: 5.0596\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 172/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/0 - 0s - loss: 0.5254 - val_loss: 5.1843\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 173/300\n",
      "1/0 - 0s - loss: 0.5310 - val_loss: 5.4209\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 174/300\n",
      "1/0 - 0s - loss: 0.4836 - val_loss: 5.4513\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 175/300\n",
      "1/0 - 0s - loss: 0.6152 - val_loss: 5.2216\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 176/300\n",
      "1/0 - 0s - loss: 0.5885 - val_loss: 5.0621\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 177/300\n",
      "1/0 - 0s - loss: 0.5314 - val_loss: 4.9513\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 178/300\n",
      "1/0 - 0s - loss: 0.5840 - val_loss: 5.0041\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 179/300\n",
      "1/0 - 0s - loss: 0.5464 - val_loss: 5.2763\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 180/300\n",
      "1/0 - 0s - loss: 0.5412 - val_loss: 5.3665\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 181/300\n",
      "1/0 - 0s - loss: 0.5202 - val_loss: 5.1805\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 182/300\n",
      "1/0 - 0s - loss: 0.5395 - val_loss: 5.0644\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 183/300\n",
      "1/0 - 0s - loss: 0.5241 - val_loss: 5.0582\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 184/300\n",
      "1/0 - 0s - loss: 0.5293 - val_loss: 4.9208\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 185/300\n",
      "1/0 - 0s - loss: 0.5509 - val_loss: 4.9205\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 186/300\n",
      "1/0 - 0s - loss: 0.5572 - val_loss: 4.9719\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 187/300\n",
      "1/0 - 0s - loss: 0.5223 - val_loss: 4.9550\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 188/300\n",
      "1/0 - 0s - loss: 0.5685 - val_loss: 4.9067\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 189/300\n",
      "1/0 - 0s - loss: 0.5264 - val_loss: 4.9456\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 190/300\n",
      "1/0 - 0s - loss: 0.5598 - val_loss: 5.1463\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 191/300\n",
      "1/0 - 0s - loss: 0.5074 - val_loss: 5.1467\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 192/300\n",
      "1/0 - 1s - loss: 0.5267 - val_loss: 5.1575\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 193/300\n",
      "1/0 - 0s - loss: 0.5591 - val_loss: 5.1758\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 194/300\n",
      "1/0 - 0s - loss: 0.5910 - val_loss: 5.1466\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 195/300\n",
      "1/0 - 0s - loss: 0.5406 - val_loss: 4.9391\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 196/300\n",
      "1/0 - 0s - loss: 0.5417 - val_loss: 5.1520\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 197/300\n",
      "1/0 - 0s - loss: 0.5211 - val_loss: 5.3147\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 198/300\n",
      "1/0 - 0s - loss: 0.4914 - val_loss: 5.2906\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 199/300\n",
      "1/0 - 0s - loss: 0.5631 - val_loss: 5.3851\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 200/300\n",
      "Model saved in new_model--200--5.2029--0.0010000.\n",
      "1/0 - 2s - loss: 0.4815 - val_loss: 5.2029\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 201/300\n",
      "1/0 - 0s - loss: 0.5247 - val_loss: 5.2489\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 202/300\n",
      "1/0 - 0s - loss: 0.5050 - val_loss: 5.0944\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 203/300\n",
      "1/0 - 0s - loss: 0.5061 - val_loss: 5.2341\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 204/300\n",
      "1/0 - 0s - loss: 0.5271 - val_loss: 5.0214\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 205/300\n",
      "1/0 - 0s - loss: 0.5497 - val_loss: 5.1339\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 206/300\n",
      "1/0 - 0s - loss: 0.5203 - val_loss: 5.1005\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 207/300\n",
      "1/0 - 0s - loss: 0.4952 - val_loss: 4.9299\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 208/300\n",
      "1/0 - 0s - loss: 0.5043 - val_loss: 4.9351\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 209/300\n",
      "1/0 - 0s - loss: 0.4697 - val_loss: 4.8493\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 210/300\n",
      "1/0 - 0s - loss: 0.4580 - val_loss: 4.9715\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 211/300\n",
      "1/0 - 0s - loss: 0.5324 - val_loss: 5.1574\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 212/300\n",
      "1/0 - 0s - loss: 0.5618 - val_loss: 4.9193\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 213/300\n",
      "1/0 - 0s - loss: 0.4980 - val_loss: 4.9427\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 214/300\n",
      "1/0 - 0s - loss: 0.4911 - val_loss: 4.8785\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 215/300\n",
      "1/0 - 0s - loss: 0.5344 - val_loss: 5.2174\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 216/300\n",
      "1/0 - 0s - loss: 0.5045 - val_loss: 5.3313\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 217/300\n",
      "1/0 - 0s - loss: 0.4823 - val_loss: 5.5468\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 218/300\n",
      "1/0 - 0s - loss: 0.5062 - val_loss: 5.4551\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 219/300\n",
      "1/0 - 0s - loss: 0.5838 - val_loss: 5.1688\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 220/300\n",
      "1/0 - 0s - loss: 0.4960 - val_loss: 5.0210\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 221/300\n",
      "1/0 - 0s - loss: 0.5988 - val_loss: 5.0705\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 222/300\n",
      "1/0 - 0s - loss: 0.4783 - val_loss: 5.0602\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 223/300\n",
      "1/0 - 0s - loss: 0.5153 - val_loss: 5.2448\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 224/300\n",
      "1/0 - 0s - loss: 0.5464 - val_loss: 5.2817\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 225/300\n",
      "1/0 - 0s - loss: 0.4956 - val_loss: 5.1476\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 226/300\n",
      "1/0 - 0s - loss: 0.5007 - val_loss: 5.0047\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 227/300\n",
      "1/0 - 0s - loss: 0.5465 - val_loss: 5.2725\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 228/300\n",
      "1/0 - 0s - loss: 0.5502 - val_loss: 5.4681\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 229/300\n",
      "1/0 - 0s - loss: 0.5177 - val_loss: 5.3715\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/0 - 0s - loss: 0.4981 - val_loss: 5.0373\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 231/300\n",
      "1/0 - 0s - loss: 0.4803 - val_loss: 4.8361\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 232/300\n",
      "1/0 - 0s - loss: 0.5262 - val_loss: 4.8297\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 233/300\n",
      "1/0 - 0s - loss: 0.4861 - val_loss: 4.7638\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 234/300\n",
      "1/0 - 0s - loss: 0.4581 - val_loss: 4.9047\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 235/300\n",
      "1/0 - 0s - loss: 0.5180 - val_loss: 4.9415\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 236/300\n",
      "1/0 - 0s - loss: 0.5472 - val_loss: 4.7851\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 237/300\n",
      "1/0 - 0s - loss: 0.5279 - val_loss: 4.8267\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 238/300\n",
      "1/0 - 0s - loss: 0.5401 - val_loss: 4.8356\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 239/300\n",
      "1/0 - 1s - loss: 0.4995 - val_loss: 4.8354\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 240/300\n",
      "1/0 - 0s - loss: 0.4545 - val_loss: 4.9673\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 241/300\n",
      "1/0 - 0s - loss: 0.4682 - val_loss: 4.9886\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 242/300\n",
      "1/0 - 0s - loss: 0.5224 - val_loss: 4.8501\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 243/300\n",
      "1/0 - 0s - loss: 0.4874 - val_loss: 4.9747\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 244/300\n",
      "1/0 - 0s - loss: 0.4554 - val_loss: 4.7579\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 245/300\n",
      "1/0 - 0s - loss: 0.5166 - val_loss: 4.6639\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 246/300\n",
      "1/0 - 0s - loss: 0.5099 - val_loss: 4.6497\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 247/300\n",
      "1/0 - 0s - loss: 0.4678 - val_loss: 4.6790\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 248/300\n",
      "1/0 - 0s - loss: 0.4904 - val_loss: 4.6133\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 249/300\n",
      "1/0 - 0s - loss: 0.5197 - val_loss: 4.8077\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 250/300\n",
      "Model saved in new_model--250--5.0619--0.0010000.\n",
      "1/0 - 2s - loss: 0.5162 - val_loss: 5.0619\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 251/300\n",
      "1/0 - 0s - loss: 0.5256 - val_loss: 5.2430\n",
      "\n",
      "Epoch 00252: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 252/300\n",
      "1/0 - 0s - loss: 0.4951 - val_loss: 5.1695\n",
      "\n",
      "Epoch 00253: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 253/300\n",
      "1/0 - 0s - loss: 0.4667 - val_loss: 4.9836\n",
      "\n",
      "Epoch 00254: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 254/300\n",
      "1/0 - 0s - loss: 0.4330 - val_loss: 4.8156\n",
      "\n",
      "Epoch 00255: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 255/300\n",
      "1/0 - 0s - loss: 0.5230 - val_loss: 4.8288\n",
      "\n",
      "Epoch 00256: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 256/300\n",
      "1/0 - 1s - loss: 0.4920 - val_loss: 4.8955\n",
      "\n",
      "Epoch 00257: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 257/300\n",
      "1/0 - 0s - loss: 0.4740 - val_loss: 4.9296\n",
      "\n",
      "Epoch 00258: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 258/300\n",
      "1/0 - 0s - loss: 0.4622 - val_loss: 4.6704\n",
      "\n",
      "Epoch 00259: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 259/300\n",
      "1/0 - 0s - loss: 0.4767 - val_loss: 4.8830\n",
      "\n",
      "Epoch 00260: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 260/300\n",
      "1/0 - 0s - loss: 0.4656 - val_loss: 4.7640\n",
      "\n",
      "Epoch 00261: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 261/300\n",
      "1/0 - 0s - loss: 0.4511 - val_loss: 4.7273\n",
      "\n",
      "Epoch 00262: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 262/300\n",
      "1/0 - 0s - loss: 0.4647 - val_loss: 4.7690\n",
      "\n",
      "Epoch 00263: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 263/300\n",
      "1/0 - 0s - loss: 0.4918 - val_loss: 5.0394\n",
      "\n",
      "Epoch 00264: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 264/300\n",
      "1/0 - 1s - loss: 0.5180 - val_loss: 4.9680\n",
      "\n",
      "Epoch 00265: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 265/300\n",
      "1/0 - 0s - loss: 0.4665 - val_loss: 4.9179\n",
      "\n",
      "Epoch 00266: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 266/300\n",
      "1/0 - 0s - loss: 0.4732 - val_loss: 4.7756\n",
      "\n",
      "Epoch 00267: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 267/300\n",
      "1/0 - 0s - loss: 0.4435 - val_loss: 4.9512\n",
      "\n",
      "Epoch 00268: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 268/300\n",
      "1/0 - 0s - loss: 0.5124 - val_loss: 5.1539\n",
      "\n",
      "Epoch 00269: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 269/300\n",
      "1/0 - 0s - loss: 0.5287 - val_loss: 4.9193\n",
      "\n",
      "Epoch 00270: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 270/300\n",
      "1/0 - 0s - loss: 0.4786 - val_loss: 4.7756\n",
      "\n",
      "Epoch 00271: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 271/300\n",
      "1/0 - 0s - loss: 0.4607 - val_loss: 4.6281\n",
      "\n",
      "Epoch 00272: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 272/300\n",
      "1/0 - 0s - loss: 0.5568 - val_loss: 4.7324\n",
      "\n",
      "Epoch 00273: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 273/300\n",
      "1/0 - 0s - loss: 0.5032 - val_loss: 5.0277\n",
      "\n",
      "Epoch 00274: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 274/300\n",
      "1/0 - 0s - loss: 0.4780 - val_loss: 4.8437\n",
      "\n",
      "Epoch 00275: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 275/300\n",
      "1/0 - 0s - loss: 0.4681 - val_loss: 4.7774\n",
      "\n",
      "Epoch 00276: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 276/300\n",
      "1/0 - 0s - loss: 0.4723 - val_loss: 4.9444\n",
      "\n",
      "Epoch 00277: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 277/300\n",
      "1/0 - 0s - loss: 0.6004 - val_loss: 4.9211\n",
      "\n",
      "Epoch 00278: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 278/300\n",
      "1/0 - 0s - loss: 0.4245 - val_loss: 4.9719\n",
      "\n",
      "Epoch 00279: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 279/300\n",
      "1/0 - 0s - loss: 0.4831 - val_loss: 4.6678\n",
      "\n",
      "Epoch 00280: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 280/300\n",
      "1/0 - 0s - loss: 0.4655 - val_loss: 4.7960\n",
      "\n",
      "Epoch 00281: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 281/300\n",
      "1/0 - 0s - loss: 0.4756 - val_loss: 4.7515\n",
      "\n",
      "Epoch 00282: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 282/300\n",
      "1/0 - 0s - loss: 0.4415 - val_loss: 4.6366\n",
      "\n",
      "Epoch 00283: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 283/300\n",
      "1/0 - 0s - loss: 0.5037 - val_loss: 4.8063\n",
      "\n",
      "Epoch 00284: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 284/300\n",
      "1/0 - 0s - loss: 0.5146 - val_loss: 4.6816\n",
      "\n",
      "Epoch 00285: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 285/300\n",
      "1/0 - 0s - loss: 0.4810 - val_loss: 4.7458\n",
      "\n",
      "Epoch 00286: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 286/300\n",
      "1/0 - 0s - loss: 0.4996 - val_loss: 4.8110\n",
      "\n",
      "Epoch 00287: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 287/300\n",
      "1/0 - 0s - loss: 0.4661 - val_loss: 4.7903\n",
      "\n",
      "Epoch 00288: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 288/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/0 - 0s - loss: 0.4626 - val_loss: 4.6397\n",
      "\n",
      "Epoch 00289: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 289/300\n",
      "1/0 - 0s - loss: 0.4634 - val_loss: 4.6556\n",
      "\n",
      "Epoch 00290: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 290/300\n",
      "1/0 - 0s - loss: 0.4451 - val_loss: 4.7905\n",
      "\n",
      "Epoch 00291: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 291/300\n",
      "1/0 - 0s - loss: 0.4530 - val_loss: 4.5426\n",
      "\n",
      "Epoch 00292: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 292/300\n",
      "1/0 - 0s - loss: 0.4961 - val_loss: 4.4927\n",
      "\n",
      "Epoch 00293: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 293/300\n",
      "1/0 - 0s - loss: 0.5290 - val_loss: 4.6992\n",
      "\n",
      "Epoch 00294: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 294/300\n",
      "1/0 - 0s - loss: 0.5060 - val_loss: 4.7625\n",
      "\n",
      "Epoch 00295: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 295/300\n",
      "1/0 - 0s - loss: 0.4839 - val_loss: 4.8715\n",
      "\n",
      "Epoch 00296: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 296/300\n",
      "1/0 - 0s - loss: 0.4448 - val_loss: 4.7431\n",
      "\n",
      "Epoch 00297: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 297/300\n",
      "1/0 - 0s - loss: 0.4831 - val_loss: 4.8077\n",
      "\n",
      "Epoch 00298: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 298/300\n",
      "1/0 - 0s - loss: 0.4539 - val_loss: 4.8746\n",
      "\n",
      "Epoch 00299: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 299/300\n",
      "1/0 - 0s - loss: 0.4323 - val_loss: 4.9629\n",
      "\n",
      "Epoch 00300: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 300/300\n",
      "Model saved in new_model--300--5.0355--0.0010000.\n",
      "1/0 - 2s - loss: 0.5183 - val_loss: 5.0355\n"
     ]
    }
   ],
   "source": [
    "model.fit(epochs              = 30,         # number of epochs\n",
    "          lr                  = 1e-3,        # initial learning rate for Adam, recommended\n",
    "          model_name          = \"new_model\", # base name to append the checkpoints with\n",
    "          checkpoint_dir      = \"\",          # save checkpoints in the notebook's directory\n",
    "          mini_epochs         = 10,          # number of sub-epochs within an epoch to trigger lr decay\n",
    "          save_period         = 50,          # checkpoint frequency (in mini_epochs)\n",
    "          lr_decay            = True,        # whether to use exponential lr decay or not\n",
    "          sch_epoch_to_start  = 500,         # mini-epoch to start lr decay (bypassed if lr_decay=False)\n",
    "          sch_lr_init         = 1e-3,        # initial lr, should be equal to lr (bypassed if lr_decay=False)\n",
    "          sch_lr_final        = 1e-6,        # final lr before finishing training (bypassed if lr_decay=False)\n",
    "          patience            = 25)          # patience for Keras' ReduceLROnPlateau (bypassed if lr_decay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n",
      "Elapsed time: 0.214 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "model.save(\"models/model_0605_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "interpreter": {
   "hash": "e78e90599c5fb8f980b5b4db672a9b193d0d2d67500e6742a058ef0e4774177f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
